"""
Safe KL Annealing Strategies
Provides numerically stable KL annealing with safety checks and adaptive mechanisms.
"""

import torch
import math
from typing import Optional


class SafeKLAnnealing:
    """Safe KL annealing with numerical stability checks"""
    
    def __init__(self, strategy='linear', max_weight=0.001, anneal_epochs=100, 
                 min_weight=1e-6, kl_target_min=0.1, kl_target_max=10.0):
        self.strategy = strategy
        self.max_weight = max_weight
        self.anneal_epochs = anneal_epochs
        self.min_weight = min_weight
        self.kl_target_min = kl_target_min
        self.kl_target_max = kl_target_max
        
        # å®‰å…¨å‚æ•°
        self.kl_div_history = []
        self.weight_history = []
        self.nan_count = 0
        self.max_nan_count = 10
        
    def get_weight(self, epoch: int, kl_div: Optional[torch.Tensor] = None) -> float:
        """è·å–å®‰å…¨çš„KLæƒé‡"""
        
        # æ£€æŸ¥KLæ•£åº¦æ˜¯å¦æœ‰æ•ˆ
        if kl_div is not None:
            kl_div_value = kl_div.item() if torch.is_tensor(kl_div) else kl_div
            
            # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
            if torch.isnan(kl_div) or torch.isinf(kl_div) or kl_div_value < 0:
                self.nan_count += 1
                print(f"âš ï¸  Invalid KL divergence: {kl_div_value}, count: {self.nan_count}")
                
                if self.nan_count > self.max_nan_count:
                    print("ğŸš¨  Too many invalid KL divergences, using emergency fallback")
                    return self.min_weight
                
                # ä½¿ç”¨å†å²å¹³å‡å€¼æˆ–é»˜è®¤å€¼
                if self.kl_div_history:
                    kl_div_value = sum(self.kl_div_history[-10:]) / min(len(self.kl_div_history), 10)
                else:
                    kl_div_value = 1.0
            
            self.kl_div_history.append(kl_div_value)
            
            # é™åˆ¶å†å²é•¿åº¦
            if len(self.kl_div_history) > 100:
                self.kl_div_history = self.kl_div_history[-50:]
        
        # æ ¹æ®ç­–ç•¥è®¡ç®—æƒé‡
        if self.strategy == 'linear':
            weight = self._linear_annealing(epoch)
        elif self.strategy == 'cosine':
            weight = self._cosine_annealing(epoch)
        elif self.strategy == 'adaptive':
            weight = self._adaptive_annealing(epoch, kl_div_value if kl_div is not None else 1.0)
        elif self.strategy == 'safe_linear':
            weight = self._safe_linear_annealing(epoch)
        else:
            weight = self._linear_annealing(epoch)
        
        # åº”ç”¨å®‰å…¨é™åˆ¶
        weight = max(self.min_weight, min(weight, self.max_weight))
        
        # æ£€æŸ¥æƒé‡æ˜¯å¦æœ‰æ•ˆ
        if math.isnan(weight) or math.isinf(weight):
            print(f"âš ï¸  Invalid weight calculated: {weight}, using fallback")
            weight = self.min_weight
        
        self.weight_history.append(weight)
        return weight
    
    def _linear_annealing(self, epoch: int) -> float:
        """çº¿æ€§é€€ç«"""
        if epoch <= self.anneal_epochs:
            return self.max_weight * (epoch / self.anneal_epochs)
        else:
            return self.max_weight
    
    def _cosine_annealing(self, epoch: int) -> float:
        """ä½™å¼¦é€€ç«"""
        if epoch <= self.anneal_epochs:
            return self.max_weight * (1 + math.cos(math.pi * epoch / self.anneal_epochs)) / 2
        else:
            return self.max_weight
    
    def _adaptive_annealing(self, epoch: int, kl_div_value: float) -> float:
        """è‡ªé€‚åº”é€€ç«"""
        base_weight = self.max_weight * (epoch / self.anneal_epochs) if epoch <= self.anneal_epochs else self.max_weight
        
        # æ ¹æ®KLæ•£åº¦è°ƒæ•´æƒé‡
        if kl_div_value < self.kl_target_min:
            # KLæ•£åº¦å¤ªå°ï¼Œå¢åŠ æƒé‡
            adjustment = 1.2
        elif kl_div_value > self.kl_target_max:
            # KLæ•£åº¦å¤ªå¤§ï¼Œå‡å°‘æƒé‡
            adjustment = 0.8
        else:
            # æ­£å¸¸èŒƒå›´
            adjustment = 1.0
        
        return base_weight * adjustment
    
    def _safe_linear_annealing(self, epoch: int) -> float:
        """å®‰å…¨çš„çº¿æ€§é€€ç«ï¼ŒåŒ…å«é¢å¤–çš„ç¨³å®šæ€§æ£€æŸ¥"""
        if epoch <= self.anneal_epochs:
            progress = epoch / self.anneal_epochs
            # ä½¿ç”¨æ›´å¹³æ»‘çš„å¢é•¿æ›²çº¿
            weight = self.max_weight * (progress ** 0.5)  # å¹³æ–¹æ ¹å¢é•¿
        else:
            weight = self.max_weight
        
        return weight
    
    def get_statistics(self) -> dict:
        """è·å–é€€ç«ç»Ÿè®¡ä¿¡æ¯"""
        if not self.kl_div_history:
            return {"message": "No history available"}
        
        return {
            "kl_div_mean": sum(self.kl_div_history) / len(self.kl_div_history),
            "kl_div_std": math.sqrt(sum((x - sum(self.kl_div_history) / len(self.kl_div_history)) ** 2 for x in self.kl_div_history) / len(self.kl_div_history)),
            "kl_div_min": min(self.kl_div_history),
            "kl_div_max": max(self.kl_div_history),
            "nan_count": self.nan_count,
            "weight_mean": sum(self.weight_history) / len(self.weight_history) if self.weight_history else 0
        }


def get_safe_kl_annealing(strategy='safe_linear', **kwargs):
    """è·å–å®‰å…¨çš„KLé€€ç«å™¨"""
    return SafeKLAnnealing(strategy=strategy, **kwargs)


# é¢„å®šä¹‰çš„å®‰å…¨é…ç½®
SAFE_CONFIGS = {
    'conservative': {
        'strategy': 'safe_linear',
        'max_weight': 0.0005,
        'anneal_epochs': 150,
        'min_weight': 1e-7
    },
    'moderate': {
        'strategy': 'linear',
        'max_weight': 0.001,
        'anneal_epochs': 100,
        'min_weight': 1e-6
    },
    'adaptive': {
        'strategy': 'adaptive',
        'max_weight': 0.002,
        'anneal_epochs': 80,
        'min_weight': 1e-6,
        'kl_target_min': 0.5,
        'kl_target_max': 5.0
    }
}


def get_predefined_safe_annealing(config_name='conservative'):
    """è·å–é¢„å®šä¹‰çš„å®‰å…¨é€€ç«é…ç½®"""
    if config_name not in SAFE_CONFIGS:
        raise ValueError(f"Unknown config: {config_name}. Available: {list(SAFE_CONFIGS.keys())}")
    
    return SafeKLAnnealing(**SAFE_CONFIGS[config_name])


if __name__ == "__main__":
    # æµ‹è¯•å®‰å…¨é€€ç«
    print("=== Testing Safe KL Annealing ===")
    
    # æµ‹è¯•æ­£å¸¸æƒ…å†µ
    annealer = get_safe_kl_annealing('safe_linear', max_weight=0.001, anneal_epochs=50)
    
    for epoch in range(60):
        kl_div = torch.tensor(1.0 + 0.5 * torch.sin(epoch * 0.1))
        weight = annealer.get_weight(epoch, kl_div)
        print(f"Epoch {epoch:2d}: KL div = {kl_div.item():.4f}, Weight = {weight:.6f}")
    
    # æµ‹è¯•å¼‚å¸¸æƒ…å†µ
    print("\n=== Testing with Invalid KL Divergence ===")
    annealer2 = get_safe_kl_annealing('adaptive', max_weight=0.002, anneal_epochs=30)
    
    for epoch in range(10):
        if epoch == 5:
            kl_div = torch.tensor(float('nan'))  # æ¨¡æ‹ŸNaN
        else:
            kl_div = torch.tensor(1.0)
        
        weight = annealer2.get_weight(epoch, kl_div)
        print(f"Epoch {epoch:2d}: KL div = {kl_div.item() if not torch.isnan(kl_div) else 'NaN'}, Weight = {weight:.6f}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n=== Statistics ===")
    stats = annealer.get_statistics()
    for key, value in stats.items():
        print(f"{key}: {value}")